{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Logistic Regression\n",
    "\n",
    "In this Notebook we will explore the performance of Logistic Regression on the datasets we already used in the Notebook for Decision Trees for Classification, Lab Notebook 6. \n",
    "\n",
    "We start with the [Spambase Dataset](http://archive.ics.uci.edu/ml/datasets/Spambase).\n",
    "\n",
    "We create a <tt>SparkSession</tt> (unless you are running in a pyspark shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Logistic Regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We load the dataset and the names of the features and label. We cache the dataframe for efficiently performing several operations to rawdata inside a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = spark.read.csv('./Data/spambase.data')\n",
    "rawdata.cache()\n",
    "ncolumns = len(rawdata.columns)\n",
    "spam_names = [spam_names.rstrip('\\n') for spam_names in open('./Data/spambase.data.names')]\n",
    "number_names = np.shape(spam_names)[0]\n",
    "for i in range(number_names):\n",
    "    local = spam_names[i]\n",
    "    colon_pos = local.find(':')\n",
    "    spam_names[i] = local[:colon_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now rename the columns using the more familiar names for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaNames = rawdata.schema.names\n",
    "spam_names[ncolumns-1] = 'labels'\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumnRenamed(schemaNames[i], spam_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the Double type from pyspark.sql.types, use the withColumn method for the dataframe and cast() the column to DoubleType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumn(spam_names[i], rawdata[spam_names[i]].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the VectorAssembler to concatenate all the features in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = spam_names[0:ncolumns-1], outputCol = 'features') \n",
    "raw_plus_vector = assembler.transform(rawdata)\n",
    "data = raw_plus_vector.select('features','labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same seed that we used in the previous Notebook to split the data into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to train the logistic regression model. But before, let us look at a list of relevant parameters. A comprehensive list of parameters for [LogisticRegression](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=logisticregression#pyspark.ml.classification.LogisticRegression) can be found in the Python API for PySpark.\n",
    "\n",
    "> **maxIter**: max number of iterations. <p>\n",
    "    **regParam**: regularization parameter ($\\ge 0$).<p>\n",
    "        **elasticNetParam**: mixing parameter for ElasticNet. It takes values in the range [0,1]. For $\\alpha=0$, the penalty is an $\\ell_2$. For $\\alpha=1$, the penalty is an $\\ell_1$.<p>\n",
    "        **family**: binomial (binary classification) or multinomial (multi-class classification). It can also be 'auto'.<p>\n",
    "            **standardization**: whether to standardize the training features before fitting the model. It can be true or false (True by default).\n",
    "            \n",
    "The function to optimise has the form\n",
    "$$\n",
    "f(\\mathbf{w}) = LL(\\mathbf{w}) + \\lambda\\Big[\\alpha\\|\\mathbf{w}\\|_1 + (1-\\alpha)\\frac{1}{2}\\|\\mathbf{w}\\|_2\\Big],\n",
    "$$\n",
    "where $LL(\\mathbf{w})$ is the logistic loss given as\n",
    "$$\n",
    "LL(\\mathbf{w}) = \\sum_{n=1}^N \\log[1+\\exp(-y_n\\mathbf{w}^{\\top}\\mathbf{x}_n)].\n",
    "$$\n",
    "\n",
    "Let us train different classifiers on the same training data. We start with logistic regression, without regularization, so $\\lambda=0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='labels', maxIter=50, regParam=0, family=\"binomial\")\n",
    "lrModel1 = lr.fit(trainingData)\n",
    "predictions = lrModel1.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g \" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the vector $\\mathbf{w}$ obtained without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_no_reg = lrModel1.coefficients.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train a second logistic regression classifier using only $\\ell_1$ regularisation ($\\lambda=0.01$ and $\\alpha=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrL1 = LogisticRegression(featuresCol='features', labelCol='labels', maxIter=50, regParam=0.01, \\\n",
    "                          elasticNetParam=1, family=\"binomial\")\n",
    "lrModelL1 = lrL1.fit(trainingData)\n",
    "predictions = lrModelL1.transform(testData)\n",
    "# With Predictions\n",
    "evaluator = MulticlassClassificationEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g \" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the vector $\\mathbf{w}$ obtained for the L1 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_L1 = lrModelL1.coefficients.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the values of the coefficients $\\mathbf{w}$ for the no regularisation case and the L1 regularisation case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.plot(w_no_reg)\n",
    "ax1.set_title('No regularisation')\n",
    "ax2.plot(w_L1)\n",
    "ax2.set_title('L1 regularisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out which features are preferred by each method. Without regularisation, the most relevant feature is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_names[np.argmax(np.abs(w_no_reg))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With L1 regularisation, the most relevant feature is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_names[np.argmax(np.abs(w_L1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last result is consistent with the most relevant feature given by the Decision Tree Classifier of Lab Notebook 6.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Try a pure L2 regularisation and an elastic net regularisation on the same data partitions from above. Compare accuracies and find the most relevant features for both cases. Are these features the same than the one obtained for L1 regularisation?\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Instead of creating a logistic regression model trying one type of regularisation at a time, create a [ParamGridBuilder](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=paramgridbuilder#pyspark.ml.tuning.ParamGridBuilder) to be used inside a [CrossValidator](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) to fine tune the best type of regularisation and the best parameters for that type of regularisation. Use five folds for the CrossValidator.\n",
    "\n",
    "A useful method for the logistic regression model is the [summary](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=logisticregressionsummary#pyspark.ml.classification.LogisticRegressionSummary) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel1.summary.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy here is different to the one we got before. Why?\n",
    "\n",
    "Other quantities that can be obtained from the summary include falsePositiveRateByLabel, precisionByLabel, recallByLabel, among others. For an exhaustive list, please read [here](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=logisticregressionsummary#pyspark.ml.classification.LogisticRegressionSummary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(lrModel1.summary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Create a logistic regression classifier that runs on the [default of credit cards](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) dataset. Several of the features in this dataset are categorical. Use the tools provided by PySpark (pyspark.ml.feature) for treating categorical variables. \n",
    "\n",
    "Note also that this dataset has a different format to the Spambase dataset above - you will need to convert from XLS format to, say, CSV, before using the data. You can use any available tool for this: for example, Excell has an export option, or there is a command line tool <tt>xls2csv</tt> available on Linux."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
